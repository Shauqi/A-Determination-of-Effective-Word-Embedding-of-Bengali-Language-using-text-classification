{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FastText + CNN + LSTM.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FZvP_g-SV47j","colab_type":"code","outputId":"2ba7e7ff-8161-41a5-b23b-d39f81d301f2","executionInfo":{"status":"ok","timestamp":1560516559342,"user_tz":-360,"elapsed":19280,"user":{"displayName":"Mahmudul Hasan","photoUrl":"","userId":"10838423032874605851"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["from google.colab import drive\n","import numpy as np\n","import pandas as pd\n","drive.mount('/content/gdrive')\n","bengali_news_after_preprocessing = pd.read_pickle('/content/gdrive/My Drive/Projects/Bengali Text Classification/Bengali_Text_after_preprocessing.pkl')\n","from sklearn.externals import joblib\n","filename = '/content/gdrive/My Drive/Projects/Bengali Text Classification/fastText_Bangla_content_full.sav'\n","loaded_model = joblib.load(filename)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=DeprecationWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Bh152mlTWClV","colab_type":"code","outputId":"095cff71-d5d9-402f-9153-3e08d6841b77","executionInfo":{"status":"ok","timestamp":1560516566859,"user_tz":-360,"elapsed":2707,"user":{"displayName":"Mahmudul Hasan","photoUrl":"","userId":"10838423032874605851"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import keras.backend as K\n","import numpy as np\n","number_of_sample, max_number_of_words, word_vector_size = 40000, 100, 32\n","temp = bengali_news_after_preprocessing.loc[:number_of_sample-1,:max_number_of_words-1]"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"BL1dPKt5WNAp","colab_type":"code","colab":{}},"source":["temp = temp.replace(['ঘস', 'ফগ', 'ঝবঃ', 'ঋন', 'ঊঘ', '\\u09e4', 'ওৎ', 'গথ', 'খঢ', 'ঝ’', 'ং', 'ঔ', 'ডড', 'গঘ','ঐব','ওঃ’ং','ুী','খষ','ি'], None)\n","X = np.zeros((number_of_sample, max_number_of_words, word_vector_size), dtype=K.floatx())\n","for i in temp.index:\n","  X[i,:,:] = loaded_model.wv[temp.loc[i,:]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kx4gwqnUWO8R","colab_type":"code","colab":{}},"source":["bengali_news = pd.read_pickle('/content/gdrive/My Drive/Projects/Bengali Text Classification/40k_bangla_newspaper_article.p')\n","bengali_news_dataframe = pd.DataFrame(bengali_news)\n","y = bengali_news_dataframe['category']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pDj7CROuWRrp","colab_type":"code","colab":{}},"source":["from sklearn import preprocessing\n","import keras\n","import numpy as np\n","le = preprocessing.LabelEncoder()\n","le.fit(y)\n","enc = le.transform(y)\n","y = keras.utils.to_categorical(enc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltHADvLlWVsg","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_val, y_train, y_val = train_test_split(X, y, shuffle = True, test_size=0.125)\n","X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, shuffle = True, test_size=0.20)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g2Z737RiWYM4","colab_type":"code","outputId":"13f5fc54-0c9a-4160-defe-524283a7e1a7","executionInfo":{"status":"ok","timestamp":1560516651837,"user_tz":-360,"elapsed":5165,"user":{"displayName":"Mahmudul Hasan","photoUrl":"","userId":"10838423032874605851"}},"colab":{"base_uri":"https://localhost:8080/","height":343}},"source":["from keras.models import Sequential\n","from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, TensorBoard\n","\n","\n","model = Sequential()\n","\n","model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same',\n","                 input_shape=(max_number_of_words, word_vector_size)))\n","model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n","model.add(Conv1D(32, kernel_size=3, activation='relu', padding='same'))\n","model.add(MaxPooling1D(pool_size=3))\n","\n","model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.3)))\n","\n","model.add(Dense(256, activation='sigmoid'))\n","model.add(Dropout(0.2))\n","model.add(Dense(256, activation='sigmoid'))\n","model.add(Dropout(0.25))\n","model.add(Dense(256, activation='sigmoid'))\n","model.add(Dropout(0.25))\n","\n","model.add(Dense(13, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0614 12:52:02.439566 139889359779712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0614 12:52:02.458703 139889359779712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0614 12:52:02.461921 139889359779712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0614 12:52:02.528251 139889359779712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","W0614 12:52:02.960746 139889359779712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","W0614 12:52:02.979024 139889359779712 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","W0614 12:52:03.963312 139889359779712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0614 12:52:03.975671 139889359779712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"9JD8UfRZWdcI","colab_type":"code","outputId":"04b2d287-8af9-470b-fe19-9b13bd68bd01","executionInfo":{"status":"ok","timestamp":1560516651838,"user_tz":-360,"elapsed":5147,"user":{"displayName":"Mahmudul Hasan","photoUrl":"","userId":"10838423032874605851"}},"colab":{"base_uri":"https://localhost:8080/","height":544}},"source":["model.summary()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d_1 (Conv1D)            (None, 100, 32)           3104      \n","_________________________________________________________________\n","conv1d_2 (Conv1D)            (None, 100, 32)           3104      \n","_________________________________________________________________\n","conv1d_3 (Conv1D)            (None, 100, 32)           3104      \n","_________________________________________________________________\n","max_pooling1d_1 (MaxPooling1 (None, 33, 32)            0         \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 512)               591872    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 256)               131328    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 256)               65792     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 256)               65792     \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 13)                3341      \n","=================================================================\n","Total params: 867,437\n","Trainable params: 867,437\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bkrt1heEWoWg","colab_type":"code","outputId":"6e54b35f-0b47-4435-9fc0-f17424d749d5","executionInfo":{"status":"ok","timestamp":1560518274575,"user_tz":-360,"elapsed":1627872,"user":{"displayName":"Mahmudul Hasan","photoUrl":"","userId":"10838423032874605851"}},"colab":{"base_uri":"https://localhost:8080/","height":3505}},"source":["history = model.fit(X_train, y_train, batch_size= 500, shuffle=True, epochs= 100, validation_data=(X_val, y_val))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["W0614 12:52:04.171535 139889359779712 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 35000 samples, validate on 4000 samples\n","Epoch 1/100\n","35000/35000 [==============================] - 21s 600us/step - loss: 2.4041 - acc: 0.2276 - val_loss: 1.9476 - val_acc: 0.3095\n","Epoch 2/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 1.9944 - acc: 0.2973 - val_loss: 1.8544 - val_acc: 0.4815\n","Epoch 3/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 1.8398 - acc: 0.4225 - val_loss: 1.6712 - val_acc: 0.4970\n","Epoch 4/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 1.6910 - acc: 0.4790 - val_loss: 1.5533 - val_acc: 0.5057\n","Epoch 5/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 1.5937 - acc: 0.4930 - val_loss: 1.4571 - val_acc: 0.5345\n","Epoch 6/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 1.4871 - acc: 0.5262 - val_loss: 1.3351 - val_acc: 0.6027\n","Epoch 7/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 1.3770 - acc: 0.5836 - val_loss: 1.2299 - val_acc: 0.6462\n","Epoch 8/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 1.2827 - acc: 0.6263 - val_loss: 1.1590 - val_acc: 0.6555\n","Epoch 9/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 1.2010 - acc: 0.6477 - val_loss: 1.0896 - val_acc: 0.6675\n","Epoch 10/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 1.1359 - acc: 0.6589 - val_loss: 1.0384 - val_acc: 0.6760\n","Epoch 11/100\n","35000/35000 [==============================] - 16s 464us/step - loss: 1.0796 - acc: 0.6757 - val_loss: 0.9798 - val_acc: 0.7102\n","Epoch 12/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 1.0270 - acc: 0.6959 - val_loss: 0.9236 - val_acc: 0.7395\n","Epoch 13/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.9833 - acc: 0.7122 - val_loss: 0.8949 - val_acc: 0.7445\n","Epoch 14/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.9459 - acc: 0.7268 - val_loss: 0.8599 - val_acc: 0.7500\n","Epoch 15/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.9185 - acc: 0.7317 - val_loss: 0.8331 - val_acc: 0.7577\n","Epoch 16/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.8940 - acc: 0.7392 - val_loss: 0.8261 - val_acc: 0.7517\n","Epoch 17/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.8759 - acc: 0.7407 - val_loss: 0.8017 - val_acc: 0.7583\n","Epoch 18/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.8571 - acc: 0.7432 - val_loss: 0.7908 - val_acc: 0.7565\n","Epoch 19/100\n","35000/35000 [==============================] - 16s 465us/step - loss: 0.8452 - acc: 0.7462 - val_loss: 0.7767 - val_acc: 0.7612\n","Epoch 20/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.8288 - acc: 0.7496 - val_loss: 0.7665 - val_acc: 0.7635\n","Epoch 21/100\n","35000/35000 [==============================] - 16s 464us/step - loss: 0.8174 - acc: 0.7512 - val_loss: 0.7597 - val_acc: 0.7612\n","Epoch 22/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.8051 - acc: 0.7551 - val_loss: 0.7511 - val_acc: 0.7658\n","Epoch 23/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.7978 - acc: 0.7547 - val_loss: 0.7397 - val_acc: 0.7740\n","Epoch 24/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.7840 - acc: 0.7590 - val_loss: 0.7357 - val_acc: 0.7743\n","Epoch 25/100\n","35000/35000 [==============================] - 16s 465us/step - loss: 0.7736 - acc: 0.7619 - val_loss: 0.7264 - val_acc: 0.7802\n","Epoch 26/100\n","35000/35000 [==============================] - 16s 465us/step - loss: 0.7670 - acc: 0.7627 - val_loss: 0.7180 - val_acc: 0.7830\n","Epoch 27/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.7559 - acc: 0.7661 - val_loss: 0.7112 - val_acc: 0.7830\n","Epoch 28/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.7462 - acc: 0.7726 - val_loss: 0.7086 - val_acc: 0.7835\n","Epoch 29/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.7381 - acc: 0.7733 - val_loss: 0.7069 - val_acc: 0.7872\n","Epoch 30/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.7326 - acc: 0.7767 - val_loss: 0.6945 - val_acc: 0.7917\n","Epoch 31/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.7254 - acc: 0.7783 - val_loss: 0.6922 - val_acc: 0.7935\n","Epoch 32/100\n","35000/35000 [==============================] - 16s 458us/step - loss: 0.7174 - acc: 0.7798 - val_loss: 0.6911 - val_acc: 0.7912\n","Epoch 33/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.7096 - acc: 0.7842 - val_loss: 0.6880 - val_acc: 0.7887\n","Epoch 34/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.7035 - acc: 0.7837 - val_loss: 0.6858 - val_acc: 0.7890\n","Epoch 35/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.6948 - acc: 0.7875 - val_loss: 0.6770 - val_acc: 0.7955\n","Epoch 36/100\n","35000/35000 [==============================] - 16s 464us/step - loss: 0.6892 - acc: 0.7882 - val_loss: 0.6719 - val_acc: 0.7955\n","Epoch 37/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.6830 - acc: 0.7924 - val_loss: 0.6676 - val_acc: 0.7975\n","Epoch 38/100\n","35000/35000 [==============================] - 16s 459us/step - loss: 0.6791 - acc: 0.7911 - val_loss: 0.6648 - val_acc: 0.7935\n","Epoch 39/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.6729 - acc: 0.7914 - val_loss: 0.6623 - val_acc: 0.7938\n","Epoch 40/100\n","35000/35000 [==============================] - 16s 458us/step - loss: 0.6696 - acc: 0.7937 - val_loss: 0.6604 - val_acc: 0.7960\n","Epoch 41/100\n","35000/35000 [==============================] - 16s 457us/step - loss: 0.6627 - acc: 0.7951 - val_loss: 0.6536 - val_acc: 0.7980\n","Epoch 42/100\n","35000/35000 [==============================] - 16s 456us/step - loss: 0.6574 - acc: 0.7976 - val_loss: 0.6481 - val_acc: 0.7990\n","Epoch 43/100\n","35000/35000 [==============================] - 16s 457us/step - loss: 0.6535 - acc: 0.7988 - val_loss: 0.6394 - val_acc: 0.8003\n","Epoch 44/100\n","35000/35000 [==============================] - 16s 456us/step - loss: 0.6466 - acc: 0.8011 - val_loss: 0.6388 - val_acc: 0.8010\n","Epoch 45/100\n","35000/35000 [==============================] - 16s 456us/step - loss: 0.6409 - acc: 0.8015 - val_loss: 0.6364 - val_acc: 0.8008\n","Epoch 46/100\n","35000/35000 [==============================] - 16s 454us/step - loss: 0.6388 - acc: 0.8025 - val_loss: 0.6298 - val_acc: 0.8032\n","Epoch 47/100\n","35000/35000 [==============================] - 16s 457us/step - loss: 0.6326 - acc: 0.8033 - val_loss: 0.6268 - val_acc: 0.8037\n","Epoch 48/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.6284 - acc: 0.8054 - val_loss: 0.6256 - val_acc: 0.8023\n","Epoch 49/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.6225 - acc: 0.8079 - val_loss: 0.6193 - val_acc: 0.8078\n","Epoch 50/100\n","35000/35000 [==============================] - 16s 459us/step - loss: 0.6167 - acc: 0.8088 - val_loss: 0.6174 - val_acc: 0.8055\n","Epoch 51/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.6128 - acc: 0.8102 - val_loss: 0.6148 - val_acc: 0.8070\n","Epoch 52/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.6126 - acc: 0.8100 - val_loss: 0.6143 - val_acc: 0.8065\n","Epoch 53/100\n","35000/35000 [==============================] - 16s 459us/step - loss: 0.6054 - acc: 0.8143 - val_loss: 0.6100 - val_acc: 0.8113\n","Epoch 54/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.6030 - acc: 0.8150 - val_loss: 0.6056 - val_acc: 0.8097\n","Epoch 55/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.5939 - acc: 0.8140 - val_loss: 0.6029 - val_acc: 0.8128\n","Epoch 56/100\n","35000/35000 [==============================] - 16s 459us/step - loss: 0.5925 - acc: 0.8173 - val_loss: 0.6008 - val_acc: 0.8085\n","Epoch 57/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.5887 - acc: 0.8194 - val_loss: 0.6026 - val_acc: 0.8113\n","Epoch 58/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.5872 - acc: 0.8187 - val_loss: 0.5976 - val_acc: 0.8100\n","Epoch 59/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.5819 - acc: 0.8203 - val_loss: 0.5936 - val_acc: 0.8128\n","Epoch 60/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.5798 - acc: 0.8215 - val_loss: 0.5938 - val_acc: 0.8125\n","Epoch 61/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.5781 - acc: 0.8213 - val_loss: 0.5859 - val_acc: 0.8160\n","Epoch 62/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.5737 - acc: 0.8232 - val_loss: 0.5862 - val_acc: 0.8160\n","Epoch 63/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.5684 - acc: 0.8238 - val_loss: 0.5823 - val_acc: 0.8192\n","Epoch 64/100\n","35000/35000 [==============================] - 16s 459us/step - loss: 0.5678 - acc: 0.8241 - val_loss: 0.5819 - val_acc: 0.8207\n","Epoch 65/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.5643 - acc: 0.8274 - val_loss: 0.5834 - val_acc: 0.8153\n","Epoch 66/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.5599 - acc: 0.8260 - val_loss: 0.5811 - val_acc: 0.8175\n","Epoch 67/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.5607 - acc: 0.8266 - val_loss: 0.5780 - val_acc: 0.8215\n","Epoch 68/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.5512 - acc: 0.8279 - val_loss: 0.5784 - val_acc: 0.8197\n","Epoch 69/100\n","35000/35000 [==============================] - 16s 459us/step - loss: 0.5539 - acc: 0.8277 - val_loss: 0.5766 - val_acc: 0.8198\n","Epoch 70/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.5517 - acc: 0.8271 - val_loss: 0.5736 - val_acc: 0.8238\n","Epoch 71/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.5491 - acc: 0.8307 - val_loss: 0.5708 - val_acc: 0.8215\n","Epoch 72/100\n","35000/35000 [==============================] - 16s 458us/step - loss: 0.5421 - acc: 0.8322 - val_loss: 0.5738 - val_acc: 0.8213\n","Epoch 73/100\n","35000/35000 [==============================] - 16s 459us/step - loss: 0.5420 - acc: 0.8315 - val_loss: 0.5701 - val_acc: 0.8203\n","Epoch 74/100\n","35000/35000 [==============================] - 16s 459us/step - loss: 0.5352 - acc: 0.8360 - val_loss: 0.5701 - val_acc: 0.8238\n","Epoch 75/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.5399 - acc: 0.8325 - val_loss: 0.5670 - val_acc: 0.8253\n","Epoch 76/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.5327 - acc: 0.8345 - val_loss: 0.5656 - val_acc: 0.8243\n","Epoch 77/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.5302 - acc: 0.8345 - val_loss: 0.5602 - val_acc: 0.8272\n","Epoch 78/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.5288 - acc: 0.8367 - val_loss: 0.5592 - val_acc: 0.8270\n","Epoch 79/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.5269 - acc: 0.8373 - val_loss: 0.5573 - val_acc: 0.8295\n","Epoch 80/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.5214 - acc: 0.8398 - val_loss: 0.5607 - val_acc: 0.8268\n","Epoch 81/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.5195 - acc: 0.8396 - val_loss: 0.5567 - val_acc: 0.8272\n","Epoch 82/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.5181 - acc: 0.8388 - val_loss: 0.5557 - val_acc: 0.8292\n","Epoch 83/100\n","35000/35000 [==============================] - 16s 464us/step - loss: 0.5181 - acc: 0.8380 - val_loss: 0.5550 - val_acc: 0.8255\n","Epoch 84/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.5172 - acc: 0.8405 - val_loss: 0.5534 - val_acc: 0.8295\n","Epoch 85/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.5121 - acc: 0.8396 - val_loss: 0.5513 - val_acc: 0.8297\n","Epoch 86/100\n","35000/35000 [==============================] - 16s 459us/step - loss: 0.5120 - acc: 0.8421 - val_loss: 0.5520 - val_acc: 0.8287\n","Epoch 87/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.5115 - acc: 0.8413 - val_loss: 0.5508 - val_acc: 0.8305\n","Epoch 88/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.5056 - acc: 0.8434 - val_loss: 0.5456 - val_acc: 0.8320\n","Epoch 89/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.5085 - acc: 0.8420 - val_loss: 0.5471 - val_acc: 0.8290\n","Epoch 90/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.5032 - acc: 0.8442 - val_loss: 0.5441 - val_acc: 0.8295\n","Epoch 91/100\n","35000/35000 [==============================] - 16s 462us/step - loss: 0.5004 - acc: 0.8435 - val_loss: 0.5461 - val_acc: 0.8297\n","Epoch 92/100\n","35000/35000 [==============================] - 16s 463us/step - loss: 0.4972 - acc: 0.8449 - val_loss: 0.5418 - val_acc: 0.8303\n","Epoch 93/100\n","35000/35000 [==============================] - 16s 461us/step - loss: 0.4936 - acc: 0.8442 - val_loss: 0.5406 - val_acc: 0.8328\n","Epoch 94/100\n","35000/35000 [==============================] - 17s 473us/step - loss: 0.4918 - acc: 0.8480 - val_loss: 0.5389 - val_acc: 0.8347\n","Epoch 95/100\n","35000/35000 [==============================] - 17s 478us/step - loss: 0.4920 - acc: 0.8458 - val_loss: 0.5396 - val_acc: 0.8303\n","Epoch 96/100\n","35000/35000 [==============================] - 16s 458us/step - loss: 0.4919 - acc: 0.8457 - val_loss: 0.5392 - val_acc: 0.8290\n","Epoch 97/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.4899 - acc: 0.8488 - val_loss: 0.5368 - val_acc: 0.8325\n","Epoch 98/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.4870 - acc: 0.8487 - val_loss: 0.5352 - val_acc: 0.8313\n","Epoch 99/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.4863 - acc: 0.8482 - val_loss: 0.5375 - val_acc: 0.8312\n","Epoch 100/100\n","35000/35000 [==============================] - 16s 460us/step - loss: 0.4823 - acc: 0.8501 - val_loss: 0.5396 - val_acc: 0.8323\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BeaGmZ4wWq3w","colab_type":"code","colab":{}},"source":["predicts = model.predict(X_test)\n","import numpy as np\n","def decode(le, one_hot):\n","    dec = np.argmax(one_hot, axis=1)\n","    return le.inverse_transform(dec)\n","y_test = decode(le, y_test)\n","y_preds = decode(le, predicts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfRZa1gYWvf8","colab_type":"code","outputId":"32c77643-a516-4449-a0a9-9ed4a65ab27c","executionInfo":{"status":"ok","timestamp":1560518276476,"user_tz":-360,"elapsed":1629757,"user":{"displayName":"Mahmudul Hasan","photoUrl":"","userId":"10838423032874605851"}},"colab":{"base_uri":"https://localhost:8080/","height":615}},"source":["from sklearn import metrics\n","\n","print(metrics.accuracy_score(y_test, y_preds))\n","\n","print(metrics.confusion_matrix(y_test, y_preds))\n","\n","print(metrics.classification_report(y_test, y_preds))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["0.843\n","[[  0   0   0   0   0   1   0   0   0   4   0   0]\n"," [  0 257   0  12   3   0   4   1   0  26   2   1]\n"," [  0   1   0   0   0   0   0   0   0   1   0   0]\n"," [  0   5   0 104   2   0   1   0   0   5   0   3]\n"," [  0   4   0   1  12   0   0   1   0   1   0   1]\n"," [  0   2   0   0   0  61   1   0   0   0   0   4]\n"," [  0   6   0   2   0   2  29   0   0   4   0   0]\n"," [  0   1   0   0   0   2   0  12   0  12   0   2]\n"," [  0   1   0   0   0   0   3   0   0   1   0   0]\n"," [  0  13   0   1   0   2   1   2   0 255   2   2]\n"," [  0   0   0   0   0   0   0   0   0   1  79   0]\n"," [  0   4   0   1   0   1   1   1   0   2   0  34]]\n","                    precision    recall  f1-score   support\n","\n","art-and-literature       0.00      0.00      0.00         5\n","        bangladesh       0.87      0.84      0.86       306\n","       durporobash       0.00      0.00      0.00         2\n","           economy       0.86      0.87      0.86       120\n","         education       0.71      0.60      0.65        20\n","     entertainment       0.88      0.90      0.89        68\n","     international       0.72      0.67      0.70        43\n","        life-style       0.71      0.41      0.52        29\n","      northamerica       0.00      0.00      0.00         5\n","           opinion       0.82      0.92      0.86       278\n","            sports       0.95      0.99      0.97        80\n","        technology       0.72      0.77      0.75        44\n","\n","          accuracy                           0.84      1000\n","         macro avg       0.60      0.58      0.59      1000\n","      weighted avg       0.83      0.84      0.84      1000\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ahSRhD2YWyE8","colab_type":"code","colab":{}},"source":["accuracy, val_accuracy = np.array(history.history[\"acc\"]), np.array(history.history[\"val_acc\"])\n","accuracy, val_accuracy = accuracy.reshape(100,1), val_accuracy.reshape(100,1)\n","accuracies = np.concatenate((accuracy,val_accuracy),axis=1)\n","np.savetxt('/content/gdrive/My Drive/Projects/Bengali Text Classification/temp.csv',accuracies,delimiter=\",\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVU2P_fwZH0W","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}